# Hockey Player Tracking System Documentation

## PROJECT GOAL
The primary goal of this project is to develop a robust system that can automatically track player positions on a hockey rink using broadcast footage. This enables advanced analytics for player movement, team formations, and game strategy analysis without requiring expensive in-arena tracking systems.

### Current Progress
- ✅ Implemented YOLOv8 model for rink feature segmentation
- ✅ Developed player detection and orientation models
- ✅ Created a 2D rink model with standardized coordinates
- ✅ Built a homography calculation system for coordinate transformation
- ✅ Implemented two-pass homography interpolation for smooth camera transitions
- ⚠️ Refining the mapping logic for faceoff circles and goal lines
- ❌ Not yet implemented robust player tracking across multiple frames

We're approximately 80% of the way toward a fully functioning system. The main focus is now on improving player identity persistence across frames.

## System Components and Files

### Shell Scripts

#### `run_tracking.sh`
**Purpose**: Main entry point for running the player tracking system
**Location**: Project root directory
**Function**: 
- Ensures the rink image is properly sized
- Calls the Python tracking script with appropriate parameters
- Processes a subset of video frames for testing and visualization
- Creates output directory with timestamp

**Usage**: 
```bash
./run_tracking.sh                   # Run with default settings (60 frames)
./run_tracking.sh --max_frames 200  # Process 200 frames
./run_tracking.sh --max_frames 900  # Process 900 frames
```

**Parameters**:
- `--max_frames`: Maximum number of frames to process (default: 60)
- Additional parameters can be modified in the script:
  - `VIDEO_PATH`: Path to input video file (default: "data/videos/PIT_vs_CHI_2016_2.mp4")
  - `SEGMENTATION_MODEL`: Path to segmentation model (default: "models/segmentation.pt")
  - `DETECTION_MODEL`: Path to detection model (default: "models/detection.pt") 
  - `ORIENTATION_MODEL`: Path to orientation model (default: "models/orient.pth")
  - `RINK_COORDINATES`: Path to rink coordinates JSON file (default: "data/rink_coordinates.json")
  - `RINK_IMAGE`: Path to rink image (default: "data/rink_resized.png")
  - `OUTPUT_DIR`: Output directory (default: "output/tracking_results_<timestamp>")
  - `START_SECOND`: Start second in video (default: 0)
  - `NUM_SECONDS`: Duration in seconds to process (default: 15)
  - `FRAME_STEP`: Process every Nth frame (default: 5)

#### `initialize_project.sh`
**Purpose**: Set up the initial project structure and dependencies
**Location**: Project root directory
**Function**:
- Creates necessary directories (data, models, output)
- Downloads required model files if needed
- Verifies system requirements

**Usage**: `./initialize_project.sh`

### Python Modules

#### `segmentation_processor.py`
**Purpose**: Process video frames to identify key rink features
**Location**: `/src/segmentation_processor.py`
**Function**:
- Loads and runs the YOLOv8 segmentation model
- Processes segmentation masks to extract features:
  - Blue lines
  - Red center line
  - Goal lines
  - Faceoff circles
- Converts segmentation masks to polygon representations
- Classifies features based on position (left/right)
- Returns structured data for homography calculation

**Key Classes**:
- `SegmentationProcessor`: Handles model inference and processing
- `FeatureExtractor`: Extracts key points from segmentation masks

**Dependencies**:
- `ultralytics`: For YOLO model inference
- `cv2`: For image processing

#### `homography_calculator.py`
**Purpose**: Calculate the transformation matrix between broadcast view and 2D rink
**Location**: `/src/homography_calculator.py`
**Function**:
- Identifies corresponding points between detected rink features and 2D model
- Maps source points (from broadcast) to destination points (on 2D rink)
- Handles special cases for:
  - Faceoff circles classification (left/right)
  - Goal line identification
  - Blue line mapping
- Calculates the homography matrix using OpenCV
- Provides debug information for visualization
- Caches and interpolates homography matrices between frames
- Performs camera zone detection (left, right, center) for better mapping

**Key Classes and Methods**:
- `HomographyCalculator`: Main class for homography calculation
  - `calculate_homography()`: Computes the homography matrix for a given frame
  - `extract_source_points()`: Extracts points from segmentation results
  - `get_destination_points()`: Gets corresponding points from 2D rink model
  - `detect_camera_zone()`: Determines which portion of the rink is shown
  - `interpolate_homography()`: Performs true interpolation between two matrices
  - `get_homography_matrix()`: Retrieves or calculates matrix for a specific frame
  - `validate_homography()`: Checks if a homography matrix is valid
  - `warp_frame()`: Transforms a frame using the homography matrix
  - `project_point_to_rink()`: Maps a point from broadcast to rink coordinates

**Dependencies**:
- `numpy`: For numerical operations
- `cv2`: For homography calculation
- `json`: For reading rink coordinates
- `collections`: For caching mechanism (deque)
- `logging`: For detailed logging

#### `player_detector.py`
**Purpose**: Detect and localize players in broadcast frames
**Location**: `/src/player_detector.py`
**Function**:
- Loads and runs player detection model
- Identifies players and goalies
- Calculates reference points for each player (usually 1/3 from bottom of bounding box)
- Filters detections based on confidence scores
- Returns structured data for player positioning

**Key Classes**:
- `PlayerDetector`: Handles model inference and detection processing

**Dependencies**:
- `torch`: For model inference
- `cv2`: For image processing

#### `orientation_model.py`
**Purpose**: Determine which direction players are facing
**Location**: `/src/orientation_model.py`
**Function**:
- Analyzes player crops to determine orientation
- Classifies players as facing left, right, or neutral
- Provides confidence scores for orientation predictions

**Key Classes**:
- `OrientationModel`: Handles model inference and orientation prediction

**Dependencies**:
- `torch`: For model inference
- `cv2`: For image processing

#### `player_tracker.py` 
**Purpose**: Orchestrate player tracking, homography calculation, and interpolation
**Location**: `/src/player_tracker.py`
**Function**:
- Integrates segmentation, homography, player detection, and orientation
- Processes frames and maintains state across the video
- Manages the two-pass homography interpolation process
- Caches homography matrices and records their source (original vs fallback)
- Implements true interpolation between valid matrices

**Key Classes and Methods**:
- `PlayerTracker`: Main class for tracking
  - `process_frame()`: Processes a single frame through the entire pipeline
  - `interpolate_missing_homography()`: Performs two-pass homography interpolation
  - `detect_players()`: Detects and tracks players in a frame
  - `get_player_positions()`: Calculates player positions on the rink

**Dependencies**:
- All other modules in the system
- `numpy`: For numerical operations
- `collections`: For data structures
- `logging`: For detailed logging

#### `process_clip.py`
**Purpose**: Main script for processing video clips
**Location**: `/src/process_clip.py`
**Function**:
- Extracts frames from video clips
- Creates the player tracker instance
- Processes each frame through the tracking pipeline
- Performs two-pass homography interpolation after all frames are processed
- Generates output files and visualizations
- Provides detailed progress information

**Key Functions**:
- `parse_args()`: Parses command-line arguments
- `main()`: Main function for processing clip

**Dependencies**:
- `cv2`: For video processing
- `numpy`: For numerical operations
- `argparse`: For command-line argument parsing
- `os`: For file system operations
- `json`: For data serialization
- `time`: For timing and performance measurement

#### `visualizer.py`
**Purpose**: Create visualizations of player tracking
**Location**: `/src/visualizer.py`
**Function**:
- Generates "quadview" visualizations showing:
  - Original broadcast frame
  - Segmented rink features
  - Detected players
  - 2D rink with player positions
- Creates HTML-based interactive visualizations
- Writes tracking data to structured formats

**Key Classes**:
- `TrackingVisualizer`: Handles visualization generation

**Dependencies**:
- `cv2`: For image manipulation
- `matplotlib`: For plotting
- `json`: For data serialization

#### `generate_quadview.py`
**Purpose**: Generate visualizations for tracking results
**Location**: `/src/generate_quadview.py`
**Function**:
- Creates visual representations of tracking results
- Generates a 2x2 grid with different views
- Shows the original frame, segmented features, and mapped positions

**Dependencies**:
- `cv2`: For image manipulation
- `numpy`: For numerical operations
- `json`: For reading tracking data

## HOMOGRAPHY INTERPOLATION SYSTEM

Our system now implements a sophisticated two-pass homography interpolation algorithm that significantly improves camera transitions and handles frames where direct homography calculation fails.

### How Homography Interpolation Works

#### First Pass (During Frame Processing)
1. The system attempts to calculate a homography matrix for each frame using detected rink features
2. Successfully calculated matrices are stored with `homography_source="original"`
3. When calculation fails, the system falls back to using the most recent valid matrix (`homography_source="fallback"`)
4. Each frame's processed information, including homography success or failure status, is recorded

#### Second Pass (True Interpolation)
After all frames are processed, the `interpolate_missing_homography` method:
1. Identifies all frames with original (successfully calculated) homography matrices
2. For each frame with a fallback matrix, finds the nearest frames with original matrices before and after it
3. Calculates an interpolation factor (t) based on the temporal position between the before and after frames
4. Performs true interpolation between the before and after matrices using:
   ```python
   # Get the before and after homography matrices
   matrix1 = frame_results[before_idx]["homography_matrix"]
   matrix2 = frame_results[after_idx]["homography_matrix"]
   
   # Calculate interpolation factor (t)
   t = (frame_idx - before_idx) / (after_idx - before_idx)
   
   # Interpolate using the weighted average
   interpolated_matrix = homography_calculator.interpolate_homography(matrix1, matrix2, t)
   ```
5. For frames that don't have a valid "after" matrix (e.g., at the end of the video), keeps the fallback
6. Updates each frame's metadata with information about how its homography was derived

### Homography Matrix Interpolation
The actual interpolation between two homography matrices happens in the `interpolate_homography` method of the `HomographyCalculator` class:

```python
def interpolate_homography(self, matrix1: np.ndarray, matrix2: np.ndarray, t: float) -> np.ndarray:
    """
    Linearly interpolate between two homography matrices.
    
    This performs true interpolation between two homography matrices, providing a 
    smooth transition between different camera views. The interpolation creates
    a weighted blend based on the temporal position between the two frames.
    
    Args:
        matrix1: First homography matrix
        matrix2: Second homography matrix
        t: Interpolation factor (0.0 to 1.0)
        
    Returns:
        Interpolated homography matrix
    """
    # Convert matrices to numpy arrays if they're lists
    if isinstance(matrix1, list):
        matrix1 = np.array(matrix1, dtype=np.float32)
    if isinstance(matrix2, list):
        matrix2 = np.array(matrix2, dtype=np.float32)
        
    # Perform weighted average
    interpolated = (1 - t) * matrix1 + t * matrix2
    
    # Normalize the interpolated matrix by scaling the last element to 1
    interpolated = interpolated / interpolated[2, 2]
    
    return interpolated
```

This provides a smooth transition between camera views and significantly improves tracking accuracy during camera movement.

## CURRENT STATUS AND ISSUES

### Recent Improvements
- ✅ Successfully implemented two-pass homography interpolation
- ✅ Added tracking of homography source (original vs. fallback)
- ✅ Improved camera zone detection for better mapping
- ✅ Enhanced destination point caching and interpolation
- ✅ Added detailed logging for the interpolation process

### Remaining Issues

#### 1. Interpolation Edge Cases
When we're close to the end of the video and don't have "after" frames with valid homography matrices, we currently just keep the fallback from the nearest previous frame. This could be improved by:
- Implementing extrapolation techniques based on recent homography trends
- Using a weighted average of multiple previous matrices
- Developing a predictive model for camera movement

#### 2. Faceoff Circle and Goal Line Classification
There are still occasional misclassifications of rink features:
```
Classified goal line at x=1092.5 as LEFT (leftmost point 641.0 is left of faceoff circle)
```
vs.
```
Classified goal line at x=1349.0 as RIGHT (no part left of any faceoff circle)
```

These inconsistencies can lead to incorrect homography calculation.

#### 3. Player Identity Persistence
The system doesn't yet maintain player identity across frames - each frame's players are detected independently. This prevents proper player tracking and trajectory analysis.

## NEXT STEPS FOR DEVELOPMENT

If you're continuing development on this project, here are the recommended next steps (in order of priority):

### 1. Improve Player Identity Persistence
Create a system to maintain player identity across frames:

```python
# Example implementation outline:
class PlayerIdentityTracker:
    def __init__(self):
        self.tracked_players = {}  # Dictionary mapping player IDs to historical positions
        self.next_id = 0
        
    def update_players(self, current_detections):
        # For each new detection, either:
        # 1. Match to an existing tracked player
        # 2. Create a new tracked player
        
        assignments = self._assign_detections_to_tracked_players(current_detections)
        
        # Update tracked players
        for player_id, detection_idx in assignments.items():
            self.tracked_players[player_id].update(current_detections[detection_idx])
            
        # Create new tracks for unassigned detections
        for i, detection in enumerate(current_detections):
            if i not in assignments.values():
                self.tracked_players[self.next_id] = TrackedPlayer(detection)
                self.next_id += 1
                
    def _assign_detections_to_tracked_players(self, detections):
        # Use Hungarian algorithm or simple nearest-neighbor matching
        # to assign current detections to existing tracked players
        # Based on position, appearance, jersey color, etc.
        pass
```

This would be implemented in the `player_tracker.py` file.

### 2. Further Enhance Homography Interpolation
Improve the homography interpolation system:

```python
# Add to HomographyCalculator class:
def extrapolate_homography(self, matrix, trend_matrices, frames_ahead):
    """Extrapolate a homography matrix based on recent trends."""
    # Calculate average rate of change in homography matrices
    changes = []
    for i in range(1, len(trend_matrices)):
        change = trend_matrices[i] - trend_matrices[i-1]
        changes.append(change)
    
    avg_change = sum(changes) / len(changes)
    
    # Apply projected change
    extrapolated = matrix + (avg_change * frames_ahead)
    extrapolated = extrapolated / extrapolated[2, 2]  # Normalize
    
    return extrapolated
```

This would improve handling of end-of-sequence frames and possibly allow for predictive tracking.

### 3. Team Identification and Jersey Number Recognition
Add team identification and jersey number recognition:

```python
# Example implementation outline:
class TeamClassifier:
    def __init__(self, team_colors_path):
        self.team_colors = self._load_team_colors(team_colors_path)
        
    def _load_team_colors(self, path):
        # Load team color definitions from JSON file
        with open(path, 'r') as f:
            return json.load(f)
            
    def classify_team(self, player_crop):
        # Extract dominant colors from player crop
        colors = self._extract_dominant_colors(player_crop)
        
        # Match colors to team definitions
        team_scores = {}
        for team, team_def in self.team_colors.items():
            team_scores[team] = self._color_match_score(colors, team_def)
            
        # Return best match
        return max(team_scores.items(), key=lambda x: x[1])[0]
        
    def _extract_dominant_colors(self, crop):
        # Use k-means clustering to extract dominant colors
        pass
        
    def _color_match_score(self, colors, team_def):
        # Calculate matching score between extracted colors and team definition
        pass
```

This would significantly enhance player tracking capabilities.

### 4. Improve Feature Classification
Refine the rink feature classification logic:

```python
# Update detection logic in homography_calculator.py:
def improved_feature_classification(self, segmentation_features):
    """Enhanced rink feature classification using multiple heuristics."""
    
    # First, establish global reference points:
    # 1. Frame center
    frame_center_x = self.broadcast_width / 2
    
    # 2. Determine overall camera angle using all detected features
    camera_angle = self._estimate_camera_angle(segmentation_features)
    
    # Now classify individual features with this contextual information
    classified_features = {}
    
    # Classify faceoff circles
    if "FaceoffCircle" in segmentation_features:
        # Use camera angle + position to determine left/right
        
    # Classify goal lines 
    if "GoalLine" in segmentation_features:
        # Use camera angle + relative positions
        
    # And so on for other features...
    
    return classified_features
```

This would reduce classification inconsistencies.

### 5. Add Advanced Analytics
Implement advanced analytics on player tracking data:

```python
# Example implementation outline:
class TrackingAnalytics:
    def __init__(self, tracking_data):
        self.tracking_data = tracking_data
        
    def calculate_player_heatmaps(self, player_id):
        """Generate heatmap of player positions."""
        positions = []
        for frame in self.tracking_data["frames"]:
            for player in frame["players"]:
                if player["id"] == player_id:
                    positions.append((player["rink_x"], player["rink_y"]))
        
        return self._generate_heatmap(positions)
        
    def calculate_team_formation(self, team, frame_range):
        """Analyze team formation during frame range."""
        pass
        
    def calculate_player_speed(self, player_id):
        """Calculate player speed over time."""
        pass
        
    def identify_zone_entries(self):
        """Identify zone entry events."""
        pass
        
    def _generate_heatmap(self, positions):
        # Generate heatmap visualization from position data
        pass
```

This would add significant value to the tracking system.

## RUNNING THE SYSTEM

Follow these steps to run the player tracking system:

### 1. Set Up the Environment

Ensure you have Python 3.7+ installed and required dependencies:
```bash
# Install required packages
pip install opencv-python numpy torch ultralytics matplotlib

# Make sure the run script is executable
chmod +x run_tracking.sh
```

### 2. Prepare Your Data

Place your hockey video in the `data/videos/` directory. The default video is `PIT_vs_CHI_2016_2.mp4`.

### 3. Run the Tracking System

For basic usage:
```bash
./run_tracking.sh
```

For processing more frames:
```bash
./run_tracking.sh --max_frames 900
```

### 4. View the Results

After processing completes, open the HTML visualization:
```bash
open output/tracking_results_*/visualization.html
```

Or view the quadview visualizations directly:
```bash
open output/tracking_results_*/quadview/
```

### 5. Analyze the JSON Output

The system generates a JSON file with all tracking data:
```bash
cat output/tracking_results_*/player_detection_data_*.json
```

This file contains all the frame-by-frame tracking information, including player positions, homography matrices, and metadata about the interpolation process.

## Technical Details

### Homography Calculation
The homography calculation is critical for accurately mapping player positions from broadcast footage to the 2D rink model. It requires:

1. At least 4 corresponding points between the broadcast view and the 2D rink model
2. Accurate classification of rink features (left/right/top/bottom)
3. Consistent mapping of source points to destination points

The current implementation in `homography_calculator.py` uses OpenCV's `findHomography` function with RANSAC algorithm for robust estimation. The main challenge is correctly identifying corresponding points between the broadcast view and the 2D rink model.

The homography matrix H is a 3x3 matrix that transforms coordinates from the broadcast view to the 2D rink model:
```
[x']   [h11 h12 h13] [x]
[y'] = [h21 h22 h23] [y]
[w']   [h31 h32 h33] [1]
```

And the final coordinates are:
```
x_rink = x' / w'
y_rink = y' / w'
```

### Two-Pass Homography Interpolation

Our system uses a two-pass approach for homography calculation:

1. **First Pass**: Calculate homography matrices for all frames directly using detected features
   - If successful, mark as `homography_source="original"`
   - If unsuccessful, use nearest previous matrix and mark as `homography_source="fallback"`

2. **Second Pass**: Perform true interpolation for frames with fallback matrices
   - For a frame with index i, find nearest original homographies at indices before < i < after
   - Calculate interpolation factor t = (i - before) / (after - before)
   - Interpolate: H_i = (1-t) * H_before + t * H_after
   - Mark as `homography_source="interpolated"`

The interpolation equation is a weighted average where the weight (t) varies from 0.0 to 1.0 based on temporal position between the two reference frames.

### Player Position Mapping
Once the homography matrix is calculated, player positions are mapped from broadcast coordinates to 2D rink coordinates using:

```python
rink_position = cv2.perspectiveTransform(np.array([[broadcast_position]]), homography_matrix)[0][0]
```

This transformation is why accurate homography calculation is critical - any errors in the homography matrix will lead to incorrect player positioning on the 2D rink.

## Configuration and Data

### `rink_coordinates.json`
**Purpose**: Defines the coordinates of key rink features in the 2D model
**Location**: `/data/rink_coordinates.json`
**Content**:
- Coordinates for blue lines, center line, goal lines
- Positions of faceoff circles, center circle
- Rink boundaries and zones

### `config.json`
**Purpose**: Configuration parameters for the tracking system
**Location**: `/config.json`
**Content**:
- Model parameters and thresholds
- Visualization settings
- Processing options

## Troubleshooting Guide

### Common Issues and Solutions

1. **"No homography matrix" warnings**:
   - Check if enough rink features are visible in the frame
   - Verify that feature classification is correct
   - Inspect source and destination point mappings

2. **Distorted player positions**:
   - Verify homography matrix calculation
   - Check for incorrect feature classifications
   - Inspect quadview visualizations for mapping issues

3. **Missing or incorrect feature detection**:
   - Adjust segmentation model confidence thresholds
   - Verify that the frame contains visible rink features
   - Check for unusual lighting or camera angles

4. **Issues with homography interpolation**:
   - Check the logs for frames being marked as "original" vs "fallback"
   - Ensure there are enough frames with successful homography calculations
   - Look for large gaps between successful frames, as these may lead to poor interpolation

## Contact and Support
For questions or contributions to the project, please contact the development team.
